{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Flatten, Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "input_shape = (32, 32, 3)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "conv1x1 = Conv2D(64, (1, 1), activation='relu', padding='same')(input_layer)\n",
        "conv1x1 = BatchNormalization()(conv1x1)\n",
        "\n",
        "conv3x3 = Conv2D(128, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "conv3x3 = BatchNormalization()(conv3x3)\n",
        "\n",
        "conv5x5 = Conv2D(32, (5, 5), activation='relu', padding='same')(input_layer)\n",
        "conv5x5 = BatchNormalization()(conv5x5)\n",
        "\n",
        "max_pool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_layer)\n",
        "\n",
        "inception_module = concatenate([conv1x1, conv3x3, conv5x5, max_pool], axis=-1)\n",
        "\n",
        "conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(inception_module)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "conv1 = Dropout(0.3)(conv1)\n",
        "\n",
        "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "conv2 = Dropout(0.3)(conv2)\n",
        "\n",
        "global_avg_pooling = GlobalAveragePooling2D()(conv2)\n",
        "\n",
        "fc1 = Dense(256, activation='relu')(global_avg_pooling)\n",
        "fc1 = Dropout(0.5)(fc1)\n",
        "\n",
        "output_layer = Dense(10, activation='softmax')(fc1)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "LDEpdVOAG0LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch Normalization:\n",
        "\n",
        "Added Batch Normalization after each convolutional layer to improve convergence and reduce internal covariate shift.\n",
        "###Dropout:\n",
        "\n",
        "Introduced dropout after convolutional and fully connected layers for regularization, reducing the risk of overfitting.\n",
        "###Global Average Pooling:\n",
        "\n",
        "Replaced the Flatten layer with Global Average Pooling to reduce the number of parameters and improve model generalization.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CNRyGzD9IBq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception Module:\n",
        "In this example, we used 1x1, 3x3, and 5x5 convolutions, along with max pooling, in parallel. This action allows the network to capture features at various receptive field sizes, enhancing its ability to recognize patterns of different scales.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRFii1EWHBVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stride Parameter:\n",
        "The stride parameter in convolutional layers affects the spatial dimensions of the feature maps. A larger stride reduces the spatial dimensions of the feature maps, leading to a more aggressive downsampling. This can be useful in reducing computational complexity and controlling overfitting. In the example, max pooling with a stride of (1, 1) is used to maintain spatial dimensions.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "moininuhHoyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional Layers:\n",
        "\n",
        "* Convolutional layers with different filter sizes (1x1, 3x3, 5x5) are used in the Inception module.\n",
        "* Max pooling with a large kernel size (3x3) and stride (1, 1) is employed.\n",
        "* These layers perform feature extraction by detecting patterns in the input image. The combination of different filter sizes allows the network to learn diverse features, contributing to improved performance."
      ],
      "metadata": {
        "id": "qsGHyDrbHuF9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}